"""
Data Integrity Tests

Project: {{ project_name }}
Domain: {{ domain }}
Auto-generated by validation framework.

Tests data integrity including schema validation, required fields,
type consistency, and referential integrity.
"""
{% if domain == "data" %}
from __future__ import annotations

import pytest


@pytest.mark.data
class TestDataIntegrity:
    """Verify data integrity constraints are maintained."""

    def test_schema_validation(self, db_connection, test_dataset) -> None:
        """Data matches defined schema."""
        for table_name, schema in test_dataset.schemas.items():
            columns = db_connection.get_table_columns(table_name)

            for expected_column in schema.columns:
                assert expected_column.name in columns, (
                    f"Table {table_name} missing column: {expected_column.name}"
                )

                actual_column = columns[expected_column.name]
                assert actual_column.data_type == expected_column.data_type, (
                    f"Column {table_name}.{expected_column.name} type mismatch: "
                    f"expected {expected_column.data_type}, got {actual_column.data_type}"
                )

    def test_required_fields_present(self, db_connection, test_dataset) -> None:
        """Required fields not null or empty."""
        for table_name, schema in test_dataset.schemas.items():
            required_columns = [c.name for c in schema.columns if c.required]

            for column in required_columns:
                null_count = db_connection.count_nulls(table_name, column)
                assert null_count == 0, (
                    f"Table {table_name} has {null_count} null values in "
                    f"required column: {column}"
                )

                # Check for empty strings in text columns
                if schema.get_column(column).data_type in ("text", "varchar", "string"):
                    empty_count = db_connection.count_empty_strings(table_name, column)
                    assert empty_count == 0, (
                        f"Table {table_name} has {empty_count} empty strings in "
                        f"required column: {column}"
                    )

    def test_data_type_consistency(self, db_connection, test_dataset) -> None:
        """Field types match specification."""
        for table_name, schema in test_dataset.schemas.items():
            sample_rows = db_connection.get_sample_rows(table_name, limit=100)

            for row in sample_rows:
                for column in schema.columns:
                    value = row.get(column.name)
                    if value is None and not column.required:
                        continue

                    # Verify type matches
                    assert column.is_valid_type(value), (
                        f"Table {table_name}, column {column.name}: "
                        f"value {value!r} does not match type {column.data_type}"
                    )

    def test_referential_integrity(self, db_connection, test_dataset) -> None:
        """Foreign key relationships valid."""
        for fk in test_dataset.foreign_keys:
            orphan_count = db_connection.count_orphan_records(
                source_table=fk.source_table,
                source_column=fk.source_column,
                target_table=fk.target_table,
                target_column=fk.target_column,
            )

            assert orphan_count == 0, (
                f"Found {orphan_count} orphan records: "
                f"{fk.source_table}.{fk.source_column} -> "
                f"{fk.target_table}.{fk.target_column}"
            )


@pytest.mark.data
class TestDataQuality:
    """Verify data quality meets standards."""

    def test_no_duplicate_records(self, db_connection, test_dataset) -> None:
        """Primary keys unique."""
        for table_name, schema in test_dataset.schemas.items():
            pk_columns = schema.primary_key_columns

            duplicate_count = db_connection.count_duplicates(
                table_name=table_name,
                columns=pk_columns,
            )

            assert duplicate_count == 0, (
                f"Table {table_name} has {duplicate_count} duplicate "
                f"primary key combinations"
            )

    def test_timestamps_sequential(self, db_connection, test_dataset) -> None:
        """Time series data ordered correctly."""
        for table_name, schema in test_dataset.schemas.items():
            timestamp_columns = [
                c.name for c in schema.columns
                if c.data_type in ("timestamp", "datetime", "date")
            ]

            for ts_column in timestamp_columns:
                if not schema.has_ordering_constraint(ts_column):
                    continue

                out_of_order = db_connection.count_out_of_order_timestamps(
                    table_name=table_name,
                    timestamp_column=ts_column,
                    ordering_key=schema.get_ordering_key(ts_column),
                )

                assert out_of_order == 0, (
                    f"Table {table_name} has {out_of_order} out-of-order "
                    f"timestamps in column {ts_column}"
                )

    def test_values_within_bounds(self, db_connection, test_dataset) -> None:
        """Numeric values in expected ranges."""
        for table_name, schema in test_dataset.schemas.items():
            for column in schema.columns:
                if not column.has_bounds:
                    continue

                out_of_bounds = db_connection.count_out_of_bounds(
                    table_name=table_name,
                    column=column.name,
                    min_value=column.min_value,
                    max_value=column.max_value,
                )

                assert out_of_bounds == 0, (
                    f"Table {table_name}, column {column.name}: "
                    f"{out_of_bounds} values outside range "
                    f"[{column.min_value}, {column.max_value}]"
                )

    def test_string_format_compliance(self, db_connection, test_dataset) -> None:
        """String fields match patterns (email, URL, etc.)."""
        for table_name, schema in test_dataset.schemas.items():
            for column in schema.columns:
                if not column.format_pattern:
                    continue

                invalid_count = db_connection.count_format_violations(
                    table_name=table_name,
                    column=column.name,
                    pattern=column.format_pattern,
                )

                assert invalid_count == 0, (
                    f"Table {table_name}, column {column.name}: "
                    f"{invalid_count} values do not match pattern "
                    f"{column.format_pattern}"
                )
{% else %}
# Skipped: domain is not "data" (domain={{ domain }})
# This template only generates tests when domain == "data"

import pytest


@pytest.mark.skip(reason="Data domain tests not enabled (domain={{ domain }})")
def test_placeholder() -> None:
    """Placeholder test - data domain not enabled."""
    pass
{% endif %}
