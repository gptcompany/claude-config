---
phase: 17-observability-dashboards
plan: 04
type: execute
wave: 2
depends_on: ["17-02"]
files_modified:
  - ~/.claude/scripts/bin/validation-report
  - ~/.claude/scripts/lib/validation-report.js
  - ~/.claude/scripts/lib/validation-report.test.js
autonomous: true
---

<objective>
Create CLI reporting tool for terminal-based validation metrics.

Purpose: Quick validation stats without opening browser - for terminal-first workflows.
Output: `validation-report` command with summary, failures, and trend views.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-observability-dashboards/17-RESEARCH.md
@.planning/phases/17-observability-dashboards/17-02-SUMMARY.md
@~/.claude/scripts/lib/validation-queries.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation-report.js library</name>
  <files>~/.claude/scripts/lib/validation-report.js</files>
  <action>
Create validation-report.js that formats query results for terminal:

1. **formatSummary(data)** - Format validation summary as table:
   ```
   Validation Summary (Last 7 days)
   ─────────────────────────────────
   Total Runs:    1,234
   Pass Rate:     94.2%
   Tier 1 Rate:   98.1%
   Avg Duration:  1.2s
   ```

2. **formatFailures(data)** - Format failing validators:
   ```
   Failing Validators
   ─────────────────────────────────
   Dimension      Pass Rate  Failures
   lint           72.3%      45
   types          81.2%      28
   coverage       85.1%      19
   ```

3. **formatTrend(data, dimension)** - ASCII sparkline or simple trend:
   ```
   Pass Rate Trend (syntax) - Last 7 days
   ─────────────────────────────────
   Mon: ████████████████████ 100%
   Tue: ██████████████████── 90%
   Wed: ████████████████████ 100%
   ```

4. **formatRecentFailures(data)** - Recent failure list:
   ```
   Recent Failures
   ─────────────────────────────────
   2026-01-26 14:32  syntax     project-a
   2026-01-26 14:15  lint       project-b
   ```

Use chalk for colors (green=good, red=bad, yellow=warning).
Import queries from validation-queries.js.

AVOID: Complex charting libraries - keep it simple with ASCII.
  </action>
  <verify>node -e "const r = require('/home/sam/.claude/scripts/lib/validation-report.js'); console.log(Object.keys(r))" shows 4 format functions</verify>
  <done>validation-report.js exports 4 formatting functions</done>
</task>

<task type="auto">
  <name>Task 2: Create validation-report CLI entry point</name>
  <files>~/.claude/scripts/bin/validation-report</files>
  <action>
Create executable CLI script at ~/.claude/scripts/bin/validation-report:

```javascript
#!/usr/bin/env node
const { program } = require('commander');
const queries = require('../lib/validation-queries');
const report = require('../lib/validation-report');

program
  .name('validation-report')
  .description('CLI validation metrics reporter')
  .version('1.0.0');

program
  .command('summary')
  .description('Show validation summary')
  .option('-d, --days <n>', 'Number of days', '7')
  .action(async (opts) => {
    const data = await queries.getValidationSummary(parseInt(opts.days));
    console.log(report.formatSummary(data));
  });

program
  .command('failures')
  .description('Show failing validators')
  .option('-d, --days <n>', 'Number of days', '7')
  .action(async (opts) => {
    const data = await queries.getFailingValidators(parseInt(opts.days));
    console.log(report.formatFailures(data));
  });

program
  .command('trend <dimension>')
  .description('Show trend for a dimension')
  .option('-d, --days <n>', 'Number of days', '7')
  .action(async (dimension, opts) => {
    const data = await queries.getTrend(dimension, parseInt(opts.days));
    console.log(report.formatTrend(data, dimension));
  });

program
  .command('recent')
  .description('Show recent failures')
  .option('-n, --limit <n>', 'Number of failures', '10')
  .action(async (opts) => {
    const data = await queries.getRecentFailures(parseInt(opts.limit));
    console.log(report.formatRecentFailures(data));
  });

program.parse();
```

Make executable: `chmod +x ~/.claude/scripts/bin/validation-report`

AVOID: Synchronous queries - use async/await properly.
  </action>
  <verify>~/.claude/scripts/bin/validation-report --help shows all 4 commands</verify>
  <done>validation-report CLI created with summary, failures, trend, recent commands</done>
</task>

<task type="auto">
  <name>Task 3: Add to PATH and create alias</name>
  <files>~/.bashrc or ~/.zshrc</files>
  <action>
1. Ensure ~/.claude/scripts/bin is in PATH (check if already added):
   ```bash
   grep -q 'claude/scripts/bin' ~/.bashrc || echo 'export PATH="$HOME/.claude/scripts/bin:$PATH"' >> ~/.bashrc
   ```

2. Create convenient alias:
   ```bash
   grep -q 'alias vr=' ~/.bashrc || echo 'alias vr="validation-report"' >> ~/.bashrc
   ```

3. Source to apply:
   ```bash
   source ~/.bashrc
   ```

4. Verify command available:
   ```bash
   which validation-report
   vr --help
   ```

AVOID: Duplicating PATH entries - check before adding.
  </action>
  <verify>which validation-report returns path and vr --help works</verify>
  <done>validation-report in PATH, vr alias configured</done>
</task>

<task type="auto">
  <name>Task 4: Add tests for report formatting</name>
  <files>~/.claude/scripts/lib/validation-report.test.js</files>
  <action>
Create validation-report.test.js:

1. Test formatSummary with mock data
2. Test formatFailures with empty array (edge case)
3. Test formatFailures with data
4. Test formatTrend produces ASCII bars
5. Test formatRecentFailures with timestamps
6. Test color codes present in output (optional)

Use Jest. Mock data should match structure from validation-queries.js.

Example test:
```javascript
describe('formatSummary', () => {
  it('formats summary data as table', () => {
    const data = {
      total_runs: 1234,
      pass_rate: 94.2,
      tier1_rate: 98.1,
      avg_duration: 1200
    };
    const output = formatSummary(data);
    expect(output).toContain('1,234');
    expect(output).toContain('94.2%');
  });
});
```

Run: `npm test -- validation-report.test.js`
  </action>
  <verify>npm test -- validation-report.test.js passes all tests</verify>
  <done>6+ tests written and passing for report formatting</done>
</task>

<task type="auto">
  <name>Task 5: Integration test with real QuestDB data</name>
  <files>none (manual verification)</files>
  <action>
Run CLI against real QuestDB data:

1. **Test summary:**
   ```bash
   validation-report summary --days 7
   ```
   Verify: Shows real numbers, not zeros or errors

2. **Test failures:**
   ```bash
   validation-report failures --days 7
   ```
   Verify: Lists validators with lowest pass rates

3. **Test trend:**
   ```bash
   validation-report trend syntax --days 7
   ```
   Verify: Shows daily trend with ASCII bars

4. **Test recent:**
   ```bash
   validation-report recent --limit 5
   ```
   Verify: Shows recent failure timestamps

5. **Test alias:**
   ```bash
   vr summary
   ```
   Verify: Alias works same as full command

Document any issues found for follow-up.
  </action>
  <verify>All 4 commands return formatted data without errors</verify>
  <done>CLI working with real QuestDB data, all commands functional</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] validation-report.js exports 4 format functions
- [ ] validation-report CLI has 4 subcommands
- [ ] CLI is in PATH and executable
- [ ] vr alias works
- [ ] Tests pass (6+ tests)
- [ ] Real data queries work against QuestDB
</verification>

<success_criteria>
- CLI runs without errors
- Output is readable and formatted
- Colors work (green/red/yellow)
- Real QuestDB data displayed correctly
- `vr summary` provides quick status check
</success_criteria>

<output>
After completion, create `.planning/phases/17-observability-dashboards/17-04-SUMMARY.md`
</output>
