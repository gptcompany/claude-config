---
phase: 17-observability-dashboards
plan: 03
type: execute
wave: 2
depends_on: ["17-02"]
files_modified:
  - ~/.claude/grafana/dashboards/validation-overview.json
  - ~/.claude/grafana/dashboards/validator-drilldown.json
  - ~/.claude/grafana/dashboards/project-comparison.json
  - /etc/grafana/provisioning/dashboards/validation.yaml
autonomous: true
---

<objective>
Create Grafana dashboard pack for validation metrics visualization.

Purpose: Historical view of validation health - trends, failures, cross-project comparison.
Output: 3 file-provisioned dashboards accessible at http://localhost:3000.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-observability-dashboards/17-RESEARCH.md
@.planning/phases/17-observability-dashboards/17-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create dashboard provisioning config</name>
  <files>/etc/grafana/provisioning/dashboards/validation.yaml</files>
  <action>
1. Create provisioning config to load dashboards from ~/.claude/grafana/dashboards/:

```yaml
apiVersion: 1
providers:
  - name: 'Validation Framework'
    orgId: 1
    type: file
    disableDeletion: false
    updateIntervalSeconds: 30
    folder: 'Validation'
    options:
      path: /home/sam/.claude/grafana/dashboards
```

2. Create directory: `mkdir -p ~/.claude/grafana/dashboards`

3. Verify Grafana can read the path (permissions)

AVOID: Using relative paths - Grafana needs absolute paths.
  </action>
  <verify>sudo cat /etc/grafana/provisioning/dashboards/validation.yaml shows valid YAML</verify>
  <done>Provisioning config created at /etc/grafana/provisioning/dashboards/validation.yaml</done>
</task>

<task type="auto">
  <name>Task 2: Create validation-overview dashboard</name>
  <files>~/.claude/grafana/dashboards/validation-overview.json</files>
  <action>
Create validation-overview.json - the main dashboard with:

1. **Top row - Key metrics (stat panels):**
   - Total runs (last 24h)
   - Overall pass rate (last 24h)
   - Tier 1 failures (critical)
   - Average duration

2. **Middle row - Time series:**
   - Pass rate over time (by tier) - use validation_daily view
   - Run volume over time

3. **Bottom row - Tables:**
   - Failing validators (lowest pass rate) - use getFailingValidators query
   - Recent failures (last 10)

Use templated datasource: `"datasource": {"type": "questdb", "uid": "${DS_QUESTDB}"}`

Add dashboard variables:
- `timeRange`: Last 24h, 7d, 30d
- `project`: All or specific project

Reference RESEARCH.md for panel JSON structure.

AVOID: Hardcoded datasource UIDs - use variables.
  </action>
  <verify>cat ~/.claude/grafana/dashboards/validation-overview.json | jq '.title' returns "Validation Overview"</verify>
  <done>validation-overview.json created with stat panels, time series, and tables</done>
</task>

<task type="auto">
  <name>Task 3: Create validator-drilldown dashboard</name>
  <files>~/.claude/grafana/dashboards/validator-drilldown.json</files>
  <action>
Create validator-drilldown.json for per-validator analysis:

1. **Variable:** `dimension` dropdown (syntax, tests, imports, lint, types, etc.)

2. **Panels:**
   - Pass rate trend for selected dimension (time series)
   - Duration trend (time series)
   - Failure distribution by hour of day (heatmap or bar)
   - Recent failures for this dimension (table with timestamps)

3. **Annotations:**
   - Mark deployment events (if available)
   - Mark alert firing times

Queries should filter by `WHERE dimension = '${dimension}'`

AVOID: Loading all dimensions at once - use variable filtering.
  </action>
  <verify>cat ~/.claude/grafana/dashboards/validator-drilldown.json | jq '.templating.list[0].name' returns "dimension"</verify>
  <done>validator-drilldown.json created with dimension variable and drilldown panels</done>
</task>

<task type="auto">
  <name>Task 4: Create project-comparison dashboard</name>
  <files>~/.claude/grafana/dashboards/project-comparison.json</files>
  <action>
Create project-comparison.json for cross-project view:

1. **Panels:**
   - Pass rate by project (bar chart)
   - Quality score by project (bar chart) - from quality_scores_daily view
   - Validation volume by project (table)
   - Trend comparison (multi-line time series, one line per project)

2. **Variable:** `projects` multi-select for filtering

3. **Queries use:**
   - `getProjectComparison(days)` pattern from validation-queries.js
   - GROUP BY project

This dashboard helps identify which projects need attention.

AVOID: Too many projects on one chart - limit to top 10 by activity.
  </action>
  <verify>cat ~/.claude/grafana/dashboards/project-comparison.json | jq '.title' returns "Project Comparison"</verify>
  <done>project-comparison.json created with cross-project comparison panels</done>
</task>

<task type="auto">
  <name>Task 5: Restart Grafana and verify dashboards</name>
  <files>none</files>
  <action>
1. Restart Grafana to load provisioned dashboards:
   `sudo systemctl restart grafana-server`

2. Wait for Grafana to start:
   `sleep 5 && curl -s http://localhost:3000/api/health`

3. Verify dashboards provisioned:
   `curl -s http://localhost:3000/api/search?folderIds=0 | jq '.[].title'`

4. Check for provisioning errors in logs:
   `sudo journalctl -u grafana-server -n 50 | grep -i "provisioning\|error"`

5. Open Grafana UI and verify dashboards appear in "Validation" folder

AVOID: Skipping log check - provisioning errors are silent in API.
  </action>
  <verify>curl http://localhost:3000/api/search shows 3 validation dashboards</verify>
  <done>Grafana restarted, 3 dashboards visible in Validation folder</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Provisioning config exists at /etc/grafana/provisioning/dashboards/validation.yaml
- [ ] ~/.claude/grafana/dashboards/ contains 3 JSON files
- [ ] Grafana shows "Validation" folder with 3 dashboards
- [ ] Dashboards load without "datasource not found" errors
- [ ] Panels show data from QuestDB (not empty)
</verification>

<success_criteria>
- 3 dashboards provisioned and visible in Grafana
- All panels render without errors
- Datasource variable works (${DS_QUESTDB})
- Time range selector affects all panels
- Can drill down from overview to validator details
</success_criteria>

<output>
After completion, create `.planning/phases/17-observability-dashboards/17-03-SUMMARY.md`
</output>
