---
phase: 17-observability-dashboards
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - ~/.claude/scripts/lib/validation-queries.js
  - ~/.claude/scripts/lib/validation-queries.test.js
autonomous: true
---

<objective>
Create QuestDB query library and materialized views for validation metrics.

Purpose: Foundation for dashboards and CLI - pre-computed aggregations for fast queries.
Output: Node.js query library + QuestDB materialized views for hourly/daily aggregations.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-observability-dashboards/17-RESEARCH.md
@~/.claude/scripts/lib/metrics.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create QuestDB materialized views for aggregations</name>
  <files>~/.claude/scripts/lib/validation-queries.js</files>
  <action>
Execute SQL to create materialized views in QuestDB:

1. **validation_hourly** - hourly aggregations:
```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS validation_hourly AS
SELECT
  timestamp_floor('h', timestamp) as hour,
  dimension,
  count() as total_runs,
  sum(passed) as passed_count,
  avg(duration) as avg_duration_ms
FROM validation
SAMPLE BY 1h;
```

2. **validation_daily** - daily aggregations:
```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS validation_daily AS
SELECT
  timestamp_floor('d', timestamp) as day,
  dimension,
  count() as total_runs,
  sum(passed) as passed_count,
  sum(case when passed = 0 then 1 else 0 end) as failed_count,
  round(sum(passed)*100.0/count(), 2) as pass_rate
FROM validation
SAMPLE BY 1d;
```

3. **quality_scores_daily** - daily quality scores:
```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS quality_scores_daily AS
SELECT
  timestamp_floor('d', timestamp) as day,
  project,
  round(avg(score_total), 2) as avg_score,
  round(min(score_total), 2) as min_score,
  round(max(score_total), 2) as max_score,
  count() as sample_count
FROM claude_quality_scores
SAMPLE BY 1d;
```

Use queryQuestDB() from metrics.js to execute.

AVOID: Dropping existing views - use IF NOT EXISTS.
  </action>
  <verify>curl "http://localhost:9000/exec?query=SHOW%20TABLES" | grep -E "validation_hourly|validation_daily|quality_scores_daily"</verify>
  <done>3 materialized views created in QuestDB</done>
</task>

<task type="auto">
  <name>Task 2: Create validation query library</name>
  <files>~/.claude/scripts/lib/validation-queries.js</files>
  <action>
Create validation-queries.js with reusable query functions:

1. **getValidationSummary(days)** - Overall stats for last N days:
   - Total runs, passed, failed
   - Pass rate by dimension
   - Average duration

2. **getFailingValidators(days)** - Worst performers:
   - Dimensions with lowest pass rates
   - Failure counts and trends

3. **getProjectComparison(days)** - Cross-project view:
   - Pass rate per project
   - Quality score averages

4. **getTrend(dimension, days)** - Time series for a dimension:
   - Daily pass rates
   - For dashboard charts

5. **getRecentFailures(limit)** - Latest failures for debugging:
   - Last N failed validations
   - With dimension and timestamp

All functions use queryQuestDB() from metrics.js. Export as CommonJS module.

AVOID: SQL injection - use parameterized queries or sanitize inputs.
  </action>
  <verify>node -e "const q = require('/home/sam/.claude/scripts/lib/validation-queries.js'); console.log(Object.keys(q))" shows all 5 functions</verify>
  <done>validation-queries.js exports 5 query functions</done>
</task>

<task type="auto">
  <name>Task 3: Add tests for query library</name>
  <files>~/.claude/scripts/lib/validation-queries.test.js</files>
  <action>
Create validation-queries.test.js with tests:

1. Test each query function returns expected structure
2. Test with days=7 (common case)
3. Test with days=1 (edge case)
4. Test getRecentFailures with limit=5
5. Mock queryQuestDB for predictable results OR use real QuestDB if available

Use Jest (already in the project). Tests should verify:
- Functions return arrays/objects
- Required fields present in results
- No errors thrown for valid inputs

Run: npm test -- validation-queries.test.js
  </action>
  <verify>npm test -- validation-queries.test.js passes all tests</verify>
  <done>8+ tests written and passing for query library</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] QuestDB shows 3 new materialized views
- [ ] validation-queries.js exports 5 functions
- [ ] All tests pass
- [ ] Can execute queries against real data: `node -e "require('./validation-queries.js').getValidationSummary(7).then(console.log)"`
</verification>

<success_criteria>
- Materialized views created in QuestDB
- Query library functional with 5 exported functions
- Tests passing (8+ tests)
- Real queries return data from QuestDB
</success_criteria>

<output>
After completion, create `.planning/phases/17-observability-dashboards/17-02-SUMMARY.md`
</output>
