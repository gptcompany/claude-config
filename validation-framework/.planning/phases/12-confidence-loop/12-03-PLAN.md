---
phase: 12-confidence-loop
plan: 03
type: execute
wave: 2
depends_on: ["12-01", "12-02"]
files_modified: [validators/multimodal/__init__.py, validators/multimodal/validator.py, validators/multimodal/score_fusion.py, validators/multimodal/tests/__init__.py, validators/multimodal/tests/test_score_fusion.py, validators/multimodal/tests/test_validator.py]
autonomous: true
---

<objective>
Create MultiModalValidator that fuses scores from visual + behavioral + a11y + perf validators.

Purpose: Combine multiple validation dimensions into a single unified confidence score that represents "done enough."
Output: Working MultiModalValidator with weighted score fusion.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-confidence-loop/12-RESEARCH.md
@.planning/phases/12-confidence-loop/12-CONTEXT.md
@.planning/phases/12-confidence-loop/12-01-SUMMARY.md
@.planning/phases/12-confidence-loop/12-02-SUMMARY.md
@~/.claude/templates/validation/orchestrator.py
</context>

<quality_requirements>
**MANDATORY for each task:**
1. Write unit tests BEFORE considering task complete
2. Target 95% coverage for new code
3. Run code-simplifier agent if code complexity > 10 or file > 200 LOC
4. Verify each task independently before proceeding to next

**Testing approach:**
- pytest with pytest-cov
- Tests in validators/multimodal/tests/
- Mock external dependencies (other validators)
- Test edge cases: missing dimensions, all dimensions, single dimension, zero weights
</quality_requirements>

<tasks>

<task type="auto">
  <name>Task 1: Create score_fusion.py with weighted fusion algorithm + tests</name>
  <files>~/.claude/templates/validation/validators/multimodal/score_fusion.py, ~/.claude/templates/validation/validators/multimodal/tests/__init__.py, ~/.claude/templates/validation/validators/multimodal/tests/test_score_fusion.py</files>
  <action>
Create weighted quasi-arithmetic mean fusion:

```python
@dataclass
class DimensionScore:
    dimension: str
    value: float  # 0.0 to 1.0
    weight: float  # Base weight
    reliability: float  # 0.0 to 1.0, how trustworthy this score is

class ScoreFusion:
    def __init__(self, base_weights: dict[str, float]):
        self.base_weights = base_weights

    def fuse(self, scores: list[DimensionScore]) -> float:
        """
        Weighted quasi-arithmetic mean with reliability adjustment.
        effective_weight = weight * reliability
        fused = sum(score * effective_weight) / sum(effective_weight)
        """

    def fuse_with_details(self, scores: list[DimensionScore]) -> dict:
        """Return fused score + breakdown per dimension."""
```

Default weights from research:
- visual_target: 0.35
- behavioral: 0.25
- accessibility: 0.20
- performance: 0.20

Handle missing dimensions gracefully (renormalize weights).

**THEN write tests in test_score_fusion.py:**
- test_fuse_all_dimensions: all 4 dimensions produce weighted average
- test_fuse_missing_dimension: weights renormalize correctly
- test_fuse_single_dimension: single score returns that value
- test_fuse_with_details: breakdown includes per-dimension contribution
- test_reliability_adjustment: low reliability reduces effective weight
- test_zero_total_weight: handles edge case gracefully
- test_boundary_values: scores at 0.0 and 1.0

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/multimodal/tests/test_score_fusion.py -v --cov=validators/multimodal/score_fusion --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/multimodal/tests/test_score_fusion.py -v --cov=validators/multimodal/score_fusion --cov-fail-under=95</verify>
  <done>ScoreFusion works, tests pass with 95%+ coverage</done>
</task>

<task type="auto">
  <name>Task 2: Create MultiModalValidator orchestrating multiple validators + tests</name>
  <files>~/.claude/templates/validation/validators/multimodal/validator.py, ~/.claude/templates/validation/validators/multimodal/__init__.py, ~/.claude/templates/validation/validators/multimodal/tests/test_validator.py</files>
  <action>
Create MultiModalValidator that runs and fuses multiple validators:
- Extends BaseValidator pattern
- dimension = "multimodal", tier = ValidationTier.MONITOR (Tier 3)
- Takes config specifying which validators to include
- Runs each validator, collects scores
- Fuses scores using ScoreFusion
- Returns unified confidence + breakdown

validate() method:
1. Run VisualTargetValidator if enabled → visual_score
2. Run BehavioralValidator if enabled → behavioral_score
3. Query existing a11y validator → a11y_score (from orchestrator)
4. Query existing perf validator → perf_score (from orchestrator)
5. Fuse all available scores
6. Return ValidationResult with confidence and details breakdown

Config options:
- enabled_dimensions: ["visual", "behavioral", "accessibility", "performance"]
- confidence_threshold: 0.90 (for "match" determination)
- custom_weights: {} (optional override)

Create __init__.py exporting MultiModalValidator, ScoreFusion, DimensionScore.

**THEN write tests in test_validator.py:**
- test_validate_all_dimensions: runs all validators, fuses scores
- test_validate_subset_dimensions: only enabled dimensions run
- test_custom_weights: respects weight override
- test_confidence_threshold: match determination correct
- test_details_breakdown: per-dimension scores in details
- test_validator_unavailable: graceful degradation if validator fails
- test_returns_validation_result: correct ValidationResult structure

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/multimodal/tests/test_validator.py -v --cov=validators/multimodal/validator --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/multimodal/tests/test_validator.py -v --cov=validators/multimodal/validator --cov-fail-under=95</verify>
  <done>MultiModalValidator works, tests pass with 95%+ coverage</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] ScoreFusion handles missing dimensions correctly
- [ ] Weights renormalize when dimensions unavailable
- [ ] MultiModalValidator returns unified confidence
- [ ] Details include per-dimension breakdown
- [ ] Graceful handling when dependent validators unavailable
</verification>

<success_criteria>

- All tasks completed
- Score fusion produces reasonable unified confidence
- Per-dimension breakdown available for debugging
- Missing dimensions handled gracefully
</success_criteria>

<output>
After completion, create `.planning/phases/12-confidence-loop/12-03-SUMMARY.md`
</output>
