---
phase: 12-confidence-loop
plan: 05
type: execute
wave: 4
depends_on: ["12-04"]
files_modified: [validators/confidence_loop/grafana_reporter.py, validators/confidence_loop/terminal_reporter.py, validators/confidence_loop/orchestrator_integration.py, validators/confidence_loop/tests/test_grafana_reporter.py, validators/confidence_loop/tests/test_terminal_reporter.py, validators/confidence_loop/tests/test_orchestrator_integration.py]
autonomous: true
---

<objective>
Create observability layer with Grafana dashboard and terminal feedback, plus orchestrator integration.

Purpose: Provide dual visibility (Grafana dashboard + terminal) into confidence loop progress and integrate with existing validation orchestrator.
Output: Working reporters and orchestrator integration for confidence-based validation.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-confidence-loop/12-RESEARCH.md
@.planning/phases/12-confidence-loop/12-CONTEXT.md
@.planning/phases/12-confidence-loop/12-04-SUMMARY.md
@~/.claude/templates/validation/orchestrator.py
</context>

<quality_requirements>
**MANDATORY for each task:**
1. Write unit tests BEFORE considering task complete
2. Target 95% coverage for new code
3. Run code-simplifier agent if code complexity > 10 or file > 200 LOC
4. Verify each task independently before proceeding to next

**Testing approach:**
- pytest with pytest-cov
- Tests in validators/confidence_loop/tests/
- Mock Grafana API calls
- Test terminal output formatting
</quality_requirements>

<tasks>

<task type="auto">
  <name>Task 1: Create terminal_reporter.py with rich progress display + tests</name>
  <files>~/.claude/templates/validation/validators/confidence_loop/terminal_reporter.py, ~/.claude/templates/validation/validators/confidence_loop/tests/test_terminal_reporter.py</files>
  <action>
Create terminal reporter for confidence loop feedback:

```python
class TerminalReporter:
    def __init__(self, use_rich: bool = True):
        self.use_rich = use_rich

    def report_iteration(self, state: LoopState, result: TerminationResult) -> None:
        """Display current iteration state in terminal."""

    def report_stage_transition(self, from_stage: RefinementStage, to_stage: RefinementStage) -> None:
        """Announce stage transition (layout → style → polish)."""

    def report_final(self, state: LoopState, result: TerminationResult) -> None:
        """Display final summary when loop terminates."""

    def format_confidence_bar(self, confidence: float, width: int = 40) -> str:
        """Create visual progress bar for confidence."""
```

Output format (using rich if available, fallback to plain text):
- Iteration number and current stage
- Confidence progress bar: [████████░░░░░░░░░░░░] 45%
- Per-dimension breakdown
- Termination reason when complete

**THEN write tests in test_terminal_reporter.py:**
- test_report_iteration: output includes state info
- test_stage_transition: announces stage change
- test_report_final: summary includes reason
- test_confidence_bar: bar width and fill correct
- test_rich_fallback: works without rich library
- test_format_percentage: 0%, 50%, 100% display correctly

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/confidence_loop/tests/test_terminal_reporter.py -v --cov=validators/confidence_loop/terminal_reporter --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/confidence_loop/tests/test_terminal_reporter.py -v --cov=validators/confidence_loop/terminal_reporter --cov-fail-under=95</verify>
  <done>TerminalReporter works, tests pass with 95%+ coverage</done>
</task>

<task type="auto">
  <name>Task 2: Create grafana_reporter.py with metrics push + tests</name>
  <files>~/.claude/templates/validation/validators/confidence_loop/grafana_reporter.py, ~/.claude/templates/validation/validators/confidence_loop/tests/test_grafana_reporter.py</files>
  <action>
Create Grafana metrics reporter:

```python
class GrafanaReporter:
    def __init__(self, grafana_url: str = None, api_key: str = None):
        self.grafana_url = grafana_url or os.getenv("GRAFANA_URL")
        self.api_key = api_key or os.getenv("GRAFANA_API_KEY")

    def push_iteration_metrics(self, state: LoopState) -> bool:
        """Push metrics for current iteration to Grafana."""

    def push_dimension_scores(self, scores: dict[str, float]) -> bool:
        """Push per-dimension scores."""

    def create_annotation(self, event: str, tags: list[str]) -> bool:
        """Create annotation for significant events (stage change, termination)."""
```

Metrics to push:
- confidence_loop_iteration (gauge)
- confidence_loop_confidence (gauge, 0-1)
- confidence_loop_stage (labeled gauge)
- confidence_loop_dimension_score (labeled gauge per dimension)

Graceful degradation if Grafana unavailable.

**THEN write tests in test_grafana_reporter.py:**
- test_push_iteration_metrics: correct payload structure
- test_push_dimension_scores: all dimensions included
- test_create_annotation: event and tags correct
- test_grafana_unavailable: graceful degradation
- test_missing_credentials: handles missing env vars
- test_api_error_handling: handles 4xx/5xx responses

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/confidence_loop/tests/test_grafana_reporter.py -v --cov=validators/confidence_loop/grafana_reporter --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/confidence_loop/tests/test_grafana_reporter.py -v --cov=validators/confidence_loop/grafana_reporter --cov-fail-under=95</verify>
  <done>GrafanaReporter works, tests pass with 95%+ coverage</done>
</task>

<task type="auto">
  <name>Task 3: Create orchestrator_integration.py + tests</name>
  <files>~/.claude/templates/validation/validators/confidence_loop/orchestrator_integration.py, ~/.claude/templates/validation/validators/confidence_loop/tests/test_orchestrator_integration.py</files>
  <action>
Create integration layer connecting confidence loop to existing orchestrator:

```python
class ConfidenceLoopOrchestrator:
    def __init__(
        self,
        base_orchestrator: ValidationOrchestrator,
        loop: ProgressiveRefinementLoop,
        terminal_reporter: TerminalReporter = None,
        grafana_reporter: GrafanaReporter = None,
    ):
        self.orchestrator = base_orchestrator
        self.loop = loop
        self.terminal = terminal_reporter or TerminalReporter()
        self.grafana = grafana_reporter

    async def run_with_confidence(
        self,
        validation_input: dict,
        confidence_threshold: float = 0.95,
    ) -> tuple[LoopState, TerminationResult]:
        """Run validation loop until confidence threshold met."""

    def get_current_confidence(self) -> float:
        """Get current unified confidence from all validators."""

    def get_dimension_breakdown(self) -> dict[str, float]:
        """Get per-dimension confidence breakdown."""
```

Integration points:
- Hooks into existing ValidationOrchestrator
- Triggers MultiModalValidator each iteration
- Reports via both TerminalReporter and GrafanaReporter
- Returns detailed results for downstream processing

Update validators/confidence_loop/__init__.py to export all classes.

**THEN write tests in test_orchestrator_integration.py:**
- test_run_with_confidence_meets_threshold: terminates on success
- test_run_with_confidence_stalls: terminates on stall
- test_reporters_called: both reporters receive updates
- test_get_current_confidence: returns fused score
- test_get_dimension_breakdown: includes all dimensions
- test_integration_with_mock_orchestrator: full integration test

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/confidence_loop/tests/test_orchestrator_integration.py -v --cov=validators/confidence_loop/orchestrator_integration --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/confidence_loop/tests/test_orchestrator_integration.py -v --cov=validators/confidence_loop/orchestrator_integration --cov-fail-under=95</verify>
  <done>ConfidenceLoopOrchestrator works, tests pass with 95%+ coverage</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] TerminalReporter tests pass with 95%+ coverage
- [ ] GrafanaReporter tests pass with 95%+ coverage
- [ ] OrchestratorIntegration tests pass with 95%+ coverage
- [ ] Combined coverage for confidence_loop module >= 95%
- [ ] Terminal output readable and informative
- [ ] Grafana metrics push correctly formatted
- [ ] All __init__.py exports updated
</verification>

<success_criteria>

- All tasks completed with tests
- Unit test coverage >= 95% for all new code
- Dual observability working (terminal + Grafana)
- Integration with existing orchestrator seamless
- Graceful degradation when dependencies unavailable
- Code is clean and simple (code-simplifier applied if needed)
</success_criteria>

<output>
After completion, create `.planning/phases/12-confidence-loop/12-05-SUMMARY.md`

Include in summary:
- Test coverage percentage for each module
- Any code simplification applied
- Integration verification results
</output>
