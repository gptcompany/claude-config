---
phase: 12-confidence-loop
plan: 04
type: execute
wave: 3
depends_on: ["12-03"]
files_modified: [validators/confidence_loop/__init__.py, validators/confidence_loop/loop_controller.py, validators/confidence_loop/termination.py, validators/confidence_loop/tests/__init__.py, validators/confidence_loop/tests/test_termination.py, validators/confidence_loop/tests/test_loop_controller.py]
autonomous: true
---

<objective>
Create ProgressiveRefinementLoop with three-stage refinement and dynamic termination.

Purpose: Implement Self-Refine pattern that continues iterating until confidence threshold met or progress stalls.
Output: Working loop controller that drives iterative refinement based on confidence.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-confidence-loop/12-RESEARCH.md
@.planning/phases/12-confidence-loop/12-CONTEXT.md
@.planning/phases/12-confidence-loop/12-03-SUMMARY.md
@~/.claude/templates/validation/orchestrator.py
</context>

<quality_requirements>
**MANDATORY for each task:**
1. Write unit tests BEFORE considering task complete
2. Target 95% coverage for new code
3. Run code-simplifier agent if code complexity > 10 or file > 200 LOC
4. Verify each task independently before proceeding to next

**Testing approach:**
- pytest with pytest-cov
- Tests in validators/confidence_loop/tests/
- Mock external dependencies (MultiModalValidator)
- Test edge cases: empty input, boundary conditions, error states
</quality_requirements>

<tasks>

<task type="auto">
  <name>Task 1: Create termination.py with dynamic termination logic + tests</name>
  <files>~/.claude/templates/validation/validators/confidence_loop/termination.py, ~/.claude/templates/validation/validators/confidence_loop/tests/__init__.py, ~/.claude/templates/validation/validators/confidence_loop/tests/test_termination.py</files>
  <action>
Create termination condition evaluator:

```python
@dataclass
class TerminationResult:
    should_stop: bool
    reason: str  # "threshold_met", "progress_stalled", "max_iterations", "manual_stop"
    confidence: float
    iterations: int
    history: list[float]  # Confidence history

class TerminationEvaluator:
    def __init__(
        self,
        confidence_threshold: float = 0.95,
        max_iterations: int = 10,
        stall_epsilon: float = 0.01,
        stall_count_limit: int = 3,
    ):
        self.history = []
        self.stall_count = 0

    def evaluate(self, current_confidence: float) -> TerminationResult:
        """
        Evaluate if loop should terminate.

        Termination conditions (in order):
        1. confidence >= threshold → "threshold_met"
        2. delta < epsilon for stall_count_limit iterations → "progress_stalled"
        3. iterations >= max_iterations → "max_iterations"
        """

    def reset(self):
        """Reset for new loop."""
```

**THEN write tests in test_termination.py:**
- test_threshold_met: confidence >= threshold returns should_stop=True
- test_progress_stalled: same confidence 3x returns should_stop=True
- test_max_iterations: 10 iterations returns should_stop=True
- test_normal_progress: increasing confidence continues
- test_reset: reset clears history
- test_edge_cases: 0.0 confidence, 1.0 confidence, negative

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/confidence_loop/tests/test_termination.py -v --cov=validators/confidence_loop/termination --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/confidence_loop/tests/test_termination.py -v --cov=validators/confidence_loop/termination --cov-fail-under=95</verify>
  <done>TerminationEvaluator works, tests pass with 95%+ coverage</done>
</task>

<task type="auto">
  <name>Task 2: Create loop_controller.py with progressive refinement + tests</name>
  <files>~/.claude/templates/validation/validators/confidence_loop/loop_controller.py, ~/.claude/templates/validation/validators/confidence_loop/__init__.py, ~/.claude/templates/validation/validators/confidence_loop/tests/test_loop_controller.py</files>
  <action>
Create ProgressiveRefinementLoop implementing Self-Refine pattern:

```python
class RefinementStage(Enum):
    LAYOUT = "layout"    # Stage 1: Get structure right
    STYLE = "style"      # Stage 2: Get appearance right
    POLISH = "polish"    # Stage 3: Fine-tune details

@dataclass
class LoopState:
    iteration: int
    stage: RefinementStage
    confidence: float
    stage_confidence: dict[RefinementStage, float]
    history: list[dict]

class ProgressiveRefinementLoop:
    def __init__(
        self,
        multimodal_validator: MultiModalValidator,
        termination_evaluator: TerminationEvaluator,
        stage_thresholds: dict[RefinementStage, float] = None,
    ):
        # Default stage thresholds
        self.stage_thresholds = stage_thresholds or {
            RefinementStage.LAYOUT: 0.80,
            RefinementStage.STYLE: 0.90,
            RefinementStage.POLISH: 0.95,
        }

    async def run_iteration(self, state: LoopState) -> tuple[LoopState, TerminationResult]:
        """Run one iteration, update state, check termination."""

    async def run(self, initial_state: LoopState = None) -> tuple[LoopState, TerminationResult]:
        """Run complete refinement loop until termination."""

    def get_current_stage(self, confidence: float) -> RefinementStage:
        """Determine current stage based on confidence."""

    def get_feedback(self, state: LoopState) -> str:
        """Generate human-readable feedback for current state."""
```

**THEN write tests in test_loop_controller.py:**
- test_stage_progression: LAYOUT → STYLE → POLISH with increasing confidence
- test_run_iteration: single iteration updates state correctly
- test_run_to_completion: loop terminates on threshold
- test_run_stalls: loop terminates on stall
- test_run_max_iterations: loop terminates on max
- test_get_feedback: returns readable string
- test_initial_state: starts at LAYOUT stage
- test_custom_thresholds: respects custom stage thresholds

Use Mock for MultiModalValidator to control confidence values.

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/confidence_loop/tests/test_loop_controller.py -v --cov=validators/confidence_loop/loop_controller --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.

Create __init__.py exporting all classes.
  </action>
  <verify>pytest validators/confidence_loop/tests/ -v --cov=validators/confidence_loop --cov-fail-under=95</verify>
  <done>ProgressiveRefinementLoop works, all tests pass with 95%+ coverage</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] TerminationEvaluator tests pass with 95%+ coverage
- [ ] LoopController tests pass with 95%+ coverage
- [ ] Combined coverage for confidence_loop module >= 95%
- [ ] Stage progression works (layout → style → polish)
- [ ] Loop produces feedback string for terminal output
- [ ] History tracked for debugging/visualization
- [ ] Code complexity acceptable (simplify if needed)
</verification>

<success_criteria>

- All tasks completed with tests
- Unit test coverage >= 95% for all new code
- Loop terminates on threshold, stall, or max iterations
- Three-stage progression works (layout → style → polish)
- State and history properly tracked
- Code is clean and simple (code-simplifier applied if needed)
</success_criteria>

<output>
After completion, create `.planning/phases/12-confidence-loop/12-04-SUMMARY.md`

Include in summary:
- Test coverage percentage
- Any code simplification applied
- Edge cases tested
</output>
