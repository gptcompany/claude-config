---
phase: 12-confidence-loop
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [validators/visual/__init__.py, validators/visual/validator.py, validators/visual/pixel_diff.py, validators/visual/perceptual.py, validators/visual/tests/__init__.py, validators/visual/tests/test_pixel_diff.py, validators/visual/tests/test_perceptual.py, validators/visual/tests/test_validator.py]
autonomous: true
---

<objective>
Create VisualTargetValidator with ODiff pixel comparison and SSIM perceptual scoring.

Purpose: Enable screenshot-driven development by comparing current UI against target screenshots and returning a confidence score.
Output: Working VisualTargetValidator that integrates with existing orchestrator pattern.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-confidence-loop/12-RESEARCH.md
@.planning/phases/12-confidence-loop/12-CONTEXT.md
@~/.claude/templates/validation/orchestrator.py
</context>

<quality_requirements>
**MANDATORY for each task:**
1. Write unit tests BEFORE considering task complete
2. Target 95% coverage for new code
3. Run code-simplifier agent if code complexity > 10 or file > 200 LOC
4. Verify each task independently before proceeding to next

**Testing approach:**
- pytest with pytest-cov
- Tests in validators/visual/tests/
- Mock external dependencies (ODiff CLI, image files)
- Test edge cases: missing files, invalid images, tool unavailable
</quality_requirements>

<tasks>

<task type="auto">
  <name>Task 1: Create pixel_diff.py with ODiff wrapper + tests</name>
  <files>~/.claude/templates/validation/validators/visual/pixel_diff.py, ~/.claude/templates/validation/validators/visual/tests/__init__.py, ~/.claude/templates/validation/validators/visual/tests/test_pixel_diff.py</files>
  <action>
Create ODiff CLI wrapper for fast pixel comparison:
- ODiffRunner class wrapping `odiff` CLI (npm install odiff-bin)
- compare(baseline: str, current: str, diff_output: str) -> dict
- Options: threshold (0.1 default), antialiasing (true), ignoreRegions
- Parse JSON output for: match, diffPercentage, diffCount
- Graceful degradation if odiff not installed (return match=False, message)
- Calculate pixel_score = 1.0 - (diffPercentage / 100.0)

Use subprocess.run with capture_output=True, timeout=30s.
Do NOT hand-roll pixel comparison - use ODiff CLI.

**THEN write tests in test_pixel_diff.py:**
- test_compare_identical: identical images return match=True, score=1.0
- test_compare_different: different images return match=False, score<1.0
- test_odiff_not_installed: graceful degradation when CLI missing
- test_missing_files: handle missing baseline/current gracefully
- test_timeout: handle slow comparison
- test_json_parsing: correctly parse ODiff output

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/visual/tests/test_pixel_diff.py -v --cov=validators/visual/pixel_diff --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/visual/tests/test_pixel_diff.py -v --cov=validators/visual/pixel_diff --cov-fail-under=95</verify>
  <done>ODiffRunner works, tests pass with 95%+ coverage</done>
</task>

<task type="auto">
  <name>Task 2: Create perceptual.py with SSIM scoring + tests</name>
  <files>~/.claude/templates/validation/validators/visual/perceptual.py, ~/.claude/templates/validation/validators/visual/tests/test_perceptual.py</files>
  <action>
Create SSIM perceptual similarity calculator:
- PerceptualComparator class
- compare(baseline: str, current: str) -> dict with ssim_score (0-1)
- Use scikit-image structural_similarity
- Handle dimension mismatch by resizing to smaller dimensions
- Convert to grayscale for comparison (more robust)
- Return: ssim_score, diff_image (optional)

from skimage.metrics import structural_similarity as ssim
from PIL import Image
import numpy as np

CRITICAL: Resize images to match before SSIM (it requires same dimensions).

**THEN write tests in test_perceptual.py:**
- test_identical_images: SSIM = 1.0
- test_different_images: SSIM < 1.0
- test_dimension_mismatch: handles different sized images
- test_missing_scikit: graceful degradation
- test_missing_files: handle missing images
- test_grayscale_conversion: works on color and grayscale

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/visual/tests/test_perceptual.py -v --cov=validators/visual/perceptual --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/visual/tests/test_perceptual.py -v --cov=validators/visual/perceptual --cov-fail-under=95</verify>
  <done>PerceptualComparator works, tests pass with 95%+ coverage</done>
</task>

<task type="auto">
  <name>Task 3: Create VisualTargetValidator with combined scoring + tests</name>
  <files>~/.claude/templates/validation/validators/visual/validator.py, ~/.claude/templates/validation/validators/visual/__init__.py, ~/.claude/templates/validation/validators/visual/tests/test_validator.py</files>
  <action>
Create VisualTargetValidator that combines ODiff + SSIM:
- Extends BaseValidator pattern from orchestrator
- dimension = "visual_target", tier = ValidationTier.MONITOR (Tier 3)
- validate(baseline: str, current: str) -> ValidationResult
- Fuse scores: confidence = (pixel_score * 0.6) + (ssim_score * 0.4)
- Store diff image path in details if mismatch
- Graceful degradation if either tool unavailable

Config options (from validation config.json):
- pixel_threshold: 0.1 (default)
- ssim_threshold: 0.95 (default)
- confidence_threshold: 0.90 (default for "match")

Create __init__.py exporting VisualTargetValidator.

**THEN write tests in test_validator.py:**
- test_validate_identical: confidence = 1.0 when images match
- test_validate_different: confidence < 1.0 when images differ
- test_score_fusion: (pixel * 0.6) + (ssim * 0.4) calculation correct
- test_graceful_degradation: works when one tool unavailable
- test_config_options: respects threshold settings
- test_returns_validation_result: correct ValidationResult structure

**VERIFY:**
```bash
cd ~/.claude/templates/validation
pytest validators/visual/tests/test_validator.py -v --cov=validators/visual/validator --cov-report=term-missing
```
Coverage must be >= 95%.

**CODE QUALITY:** If file > 200 LOC or complexity high, run code-simplifier.
  </action>
  <verify>pytest validators/visual/tests/test_validator.py -v --cov=validators/visual/validator --cov-fail-under=95</verify>
  <done>VisualTargetValidator works, tests pass with 95%+ coverage</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All three modules importable without errors
- [ ] ODiffRunner handles missing odiff CLI gracefully
- [ ] PerceptualComparator handles image dimension mismatches
- [ ] VisualTargetValidator returns confidence score 0-1
- [ ] No external dependencies crash on import (graceful degradation)
</verification>

<success_criteria>

- All tasks completed
- VisualTargetValidator follows BaseValidator pattern
- Confidence score combines pixel + SSIM correctly
- Graceful degradation when tools unavailable
</success_criteria>

<output>
After completion, create `.planning/phases/12-confidence-loop/12-01-SUMMARY.md`
</output>
