---
phase: 13-ecc-integration
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - ~/.claude/templates/validation/validators/ecc/e2e_validator.py
  - ~/.claude/templates/validation/validators/ecc/security_enhanced.py
  - ~/.claude/templates/validation/validators/ecc/tdd_validator.py
  - ~/.claude/templates/validation/validators/ecc/eval_validator.py
  - ~/.claude/templates/validation/validators/ecc/tests/
autonomous: true
---

<objective>
Port ECC agents (e2e-runner, security-reviewer, tdd-guide, eval-harness) as Python validators.

Purpose: Gain ECC's specialized domain knowledge (E2E testing, OWASP security, TDD enforcement, pass@k metrics) while maintaining our tiered execution model.
Output: Four new validators in validators/ecc/ with tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/13-ecc-integration/13-RESEARCH.md
@.planning/phases/13-ecc-integration/13-01-SUMMARY.md
@~/.claude/templates/validation/validators/ecc/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create E2EValidator (wraps e2e-runner)</name>
  <files>~/.claude/templates/validation/validators/ecc/e2e_validator.py, ~/.claude/templates/validation/validators/ecc/tests/test_e2e_validator.py</files>
  <action>
Create `e2e_validator.py` implementing the E2EValidator from research code examples:

1. Extend ECCValidatorBase
2. dimension = "e2e_validation", tier = ValidationTier.BLOCKER (Tier 1)
3. Check for playwright.config.ts existence (skip if not found)
4. Run `npx playwright test --reporter=json`
5. Parse JSON output for: total, passed, failed, flaky counts
6. Pass if failed == 0 (allow flaky tests)
7. Return ValidationResult with details and fix_suggestion if failed

Create test file with:
- Test for project without playwright (should skip cleanly)
- Test for mocked successful run
- Test for mocked failed run with fix suggestion

Use the code example from 13-RESEARCH.md as starting point.

DO NOT install Playwright globally - just test the validator logic with mocked subprocess.
  </action>
  <verify>python -c "from validators.ecc.e2e_validator import E2EValidator; print(E2EValidator.dimension)"</verify>
  <done>E2EValidator validates Playwright tests, returns proper ValidationResult</done>
</task>

<task type="auto">
  <name>Task 2: Create SecurityEnhancedValidator (OWASP checks from security-reviewer)</name>
  <files>~/.claude/templates/validation/validators/ecc/security_enhanced.py, ~/.claude/templates/validation/validators/ecc/tests/test_security_enhanced.py</files>
  <action>
Create `security_enhanced.py` with OWASP Top 10 pattern checks from ECC security-reviewer:

1. Extend ECCValidatorBase
2. dimension = "security_enhanced", tier = ValidationTier.BLOCKER
3. Implement checks for:
   - A01 (Broken Access Control): grep for @requires_auth, @login_required decorators
   - A03 (Injection): grep for f-string SQL queries
   - A07 (XSS): grep for innerHTML usage
   - A09 (Logging failures): grep for bare except without logging
4. Each check returns a list of issues
5. Aggregate all issues into ValidationResult

These are grep-based heuristics, not full security scanning (we already have bandit/Trivy for that).

Test file with:
- Test with clean code (no issues)
- Test with each vulnerability pattern (mock grep output)
  </action>
  <verify>python -c "from validators.ecc.security_enhanced import SecurityEnhancedValidator; print(SecurityEnhancedValidator.dimension)"</verify>
  <done>SecurityEnhancedValidator adds OWASP pattern checks to existing security validation</done>
</task>

<task type="auto">
  <name>Task 3: Create TDDValidator and EvalValidator</name>
  <files>~/.claude/templates/validation/validators/ecc/tdd_validator.py, ~/.claude/templates/validation/validators/ecc/eval_validator.py, ~/.claude/templates/validation/validators/ecc/tests/test_tdd_validator.py, ~/.claude/templates/validation/validators/ecc/tests/test_eval_validator.py</files>
  <action>
Create two validators:

**TDDValidator (tdd_validator.py):**
1. dimension = "tdd_compliance", tier = ValidationTier.WARNING (Tier 2)
2. Check for test files corresponding to source files:
   - For each src/*.py, check if tests/test_*.py exists
   - Calculate coverage ratio: test_files / source_files
3. Pass if coverage_ratio >= 0.8 (80% of files have tests)
4. Return details with list of files missing tests

**EvalValidator (eval_validator.py):**
1. dimension = "eval_metrics", tier = ValidationTier.MONITOR (Tier 3)
2. Check for `.claude/evals/` directory
3. If exists, read latest eval results and extract pass@k metrics
4. Always passes (Tier 3 is monitoring only)
5. Emit metrics for Grafana integration

Test files with mocked filesystem checks.
  </action>
  <verify>python -c "from validators.ecc import TDDValidator, EvalValidator; print(TDDValidator.dimension, EvalValidator.dimension)"</verify>
  <done>TDDValidator checks test coverage, EvalValidator monitors pass@k metrics</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All 4 validators import cleanly: E2EValidator, SecurityEnhancedValidator, TDDValidator, EvalValidator
- [ ] Each validator has tests in tests/ subdirectory
- [ ] pytest validators/ecc/tests/ passes
- [ ] Validators follow ECCValidatorBase pattern from Plan 13-01
</verification>

<success_criteria>

- Four new validators implemented and tested
- Tier mapping matches research: E2E→Tier1, SecurityEnhanced→Tier1, TDD→Tier2, Eval→Tier3
- Each validator has agent attribute linking to ECC source
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/13-ecc-integration/13-02-SUMMARY.md`
</output>
