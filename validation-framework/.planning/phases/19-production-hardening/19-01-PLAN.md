---
phase: 19-production-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - ~/.claude/templates/validation/tests/e2e/__init__.py
  - ~/.claude/templates/validation/tests/e2e/conftest.py
  - ~/.claude/templates/validation/tests/e2e/test_spawn_agent_live.py
  - ~/.claude/templates/validation/tests/e2e/test_full_validation.py
autonomous: true
domain: python-asyncio
---

<objective>
Create E2E test foundation with live spawn_agent() tests that validate the real claude-flow integration.

Purpose: Without E2E tests, we're hardening blind. These tests exercise the actual MCP integration, proving the validation pipeline works end-to-end before we add resilience layers.

Output: `~/.claude/templates/validation/tests/e2e/` directory with 10 passing E2E tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-production-hardening/19-RESEARCH.md
@.planning/phases/19-production-hardening/19-CONTEXT.md
@~/.claude/templates/validation/orchestrator.py
@~/.claude/templates/validation/tests/test_orchestrator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create E2E test directory and fixtures</name>
  <files>~/.claude/templates/validation/tests/e2e/__init__.py, ~/.claude/templates/validation/tests/e2e/conftest.py</files>
  <action>
1. Create `tests/e2e/__init__.py` (empty marker)
2. Create `tests/e2e/conftest.py` with:
   - `@pytest_asyncio.fixture` for MCP client connection using tenacity retry (3 attempts, exponential backoff)
   - `@pytest.fixture` for temp project directory (creates valid .claude/validation/config.json)
   - `@pytest.fixture` for mock validation target (simple Python file with intentional lint errors)
   - Environment variable check: `VALIDATION_E2E_ENABLED` - skip all E2E if not set to "true"
   - Proper cleanup in fixtures (close connections, remove temp dirs)

Use pytest-asyncio 1.x patterns:
- No `event_loop` fixture
- Use `pytest.ini` setting: `asyncio_default_fixture_loop_scope = function`
- Use `@pytest_asyncio.fixture` for async fixtures

Do NOT import non-existent modules. If claude-flow MCP not available, tests should skip gracefully.
  </action>
  <verify>pytest tests/e2e/ --collect-only shows fixtures registered</verify>
  <done>E2E test directory exists with conftest.py defining reusable fixtures</done>
</task>

<task type="auto">
  <name>Task 2: Create spawn_agent integration tests</name>
  <files>~/.claude/templates/validation/tests/e2e/test_spawn_agent_live.py</files>
  <action>
Create test file with 5 tests for spawn_agent() integration:

1. `test_spawn_agent_basic` - Spawn a simple agent, verify it returns a result
2. `test_spawn_agent_timeout` - Agent with 5s timeout, verify TimeoutError on slow task
3. `test_spawn_agent_error_propagation` - Agent failure propagates correctly
4. `test_spawn_agent_concurrent` - 3 agents in parallel, all complete
5. `test_spawn_agent_retry_on_connection` - Connection fails, retry succeeds

All tests MUST:
- Use `@pytest.mark.skipif(not os.environ.get("VALIDATION_E2E_ENABLED"))`
- Use `@pytest.mark.asyncio`
- Use tenacity retry on MCP connection (from conftest fixture)
- Have clear assertions with meaningful error messages
- Clean up any spawned agents in finally/teardown

Follow pytest-asyncio 1.x patterns. Avoid flaky tests by using tenacity for retries.
  </action>
  <verify>VALIDATION_E2E_ENABLED=true pytest tests/e2e/test_spawn_agent_live.py -v shows 5 tests</verify>
  <done>5 spawn_agent E2E tests implemented with proper skip conditions</done>
</task>

<task type="auto">
  <name>Task 3: Create full validation flow E2E tests</name>
  <files>~/.claude/templates/validation/tests/e2e/test_full_validation.py</files>
  <action>
Create test file with 5 tests for full validation flow:

1. `test_validation_happy_path` - Clean project validates, all tiers pass
2. `test_validation_tier1_blocks` - Tier 1 failure blocks overall result
3. `test_validation_tier2_warns` - Tier 2 failure warns but doesn't block
4. `test_validation_tier3_monitors` - Tier 3 emits metrics (check QuestDB if available)
5. `test_validation_partial_results` - One validator fails, others still complete

Tests should:
- Use the temp project fixture from conftest
- Create realistic validation scenarios (lint errors, type errors, etc.)
- Assert on ValidationReport structure
- Verify `blocked` flag behavior per tier
- Use `@pytest.mark.skipif` for tests requiring external services (QuestDB)

Import only from orchestrator.py, no new dependencies.
  </action>
  <verify>VALIDATION_E2E_ENABLED=true pytest tests/e2e/test_full_validation.py -v shows 5 tests</verify>
  <done>5 full validation E2E tests with realistic scenarios</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pytest tests/e2e/ --collect-only` shows 10 tests collected
- [ ] `pytest tests/e2e/ -v --ignore-glob='*test_spawn*'` passes (tests that don't need MCP)
- [ ] E2E tests skip gracefully when `VALIDATION_E2E_ENABLED` not set
- [ ] No flaky tests - run 3x and all pass
</verification>

<success_criteria>
- All 10 E2E tests implemented
- Tests skip gracefully without MCP/E2E env vars
- Zero flaky tests (deterministic pass/fail)
- Tests follow pytest-asyncio 1.x patterns
- Fixtures clean up resources properly
</success_criteria>

<output>
After completion, create `.planning/phases/19-production-hardening/19-01-SUMMARY.md`
</output>
