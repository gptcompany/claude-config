---
phase: 15-skills-port
plan: 04
type: execute
wave: 3
depends_on: ["15-02"]
files_modified:
  - ~/.claude/scripts/hooks/skills/eval/eval-harness.js
  - ~/.claude/scripts/hooks/skills/eval/eval-storage.js
  - ~/.claude/commands/eval/run.md
  - ~/.claude/commands/eval/report.md
autonomous: true
---

<objective>
Port eval-harness skill with pass@k metrics tracking.

Purpose: Track pass@k metrics for test runs across sessions, enabling analysis of first-attempt vs multi-attempt success rates.
Output: Eval harness (~250 LOC), storage library (~100 LOC), 2 commands, QuestDB export, 20+ tests
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/15-skills-port/15-RESEARCH.md

# ECC source reference:
# /media/sam/1TB/everything-claude-code/skills/eval-harness/SKILL.md

# Existing metrics infrastructure:
@~/.claude/scripts/lib/metrics.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Eval Storage Library</name>
  <files>~/.claude/scripts/hooks/skills/eval/eval-storage.js</files>
  <action>
Create eval results storage library (~100 LOC):

```javascript
const fs = require('fs');
const path = require('path');
const os = require('os');
const { exportToQuestDB } = require('../../lib/metrics');

const EVAL_DIR = path.join(os.homedir(), '.claude', 'evals');
const RESULTS_FILE = path.join(EVAL_DIR, 'results.json');

function ensureDir() {
  if (!fs.existsSync(EVAL_DIR)) {
    fs.mkdirSync(EVAL_DIR, { recursive: true });
  }
}

function loadResults() {
  ensureDir();
  try {
    return JSON.parse(fs.readFileSync(RESULTS_FILE, 'utf8'));
  } catch {
    return { runs: [], summary: {} };
  }
}

function saveResults(results) {
  ensureDir();
  fs.writeFileSync(RESULTS_FILE, JSON.stringify(results, null, 2));
}

function recordRun(run) {
  const results = loadResults();

  // Add run with metadata
  const enrichedRun = {
    ...run,
    id: `run-${Date.now()}`,
    timestamp: new Date().toISOString(),
    sessionId: process.env.CLAUDE_SESSION_ID,
  };

  results.runs.push(enrichedRun);

  // Update summary
  updateSummary(results);

  saveResults(results);

  // Export to QuestDB async
  exportToQuestDB('claude_eval_runs', {
    passed: run.passed ? 1 : 0,
    total: run.total,
    attempt: run.attempt,
    duration_ms: run.duration,
  }, {
    project: run.project,
    test_suite: run.suite,
  }).catch(() => {});

  return enrichedRun;
}

function updateSummary(results) {
  const recentRuns = results.runs.slice(-100); // Last 100 runs

  // Calculate pass@k
  const byAttempt = {};
  recentRuns.forEach(run => {
    const k = run.attempt || 1;
    if (!byAttempt[k]) byAttempt[k] = { passed: 0, total: 0 };
    byAttempt[k].total++;
    if (run.passed) byAttempt[k].passed++;
  });

  results.summary = {
    totalRuns: recentRuns.length,
    passAt: {},
    lastUpdated: new Date().toISOString(),
  };

  // pass@1, pass@2, etc.
  Object.keys(byAttempt).forEach(k => {
    results.summary.passAt[`pass@${k}`] = (byAttempt[k].passed / byAttempt[k].total * 100).toFixed(1) + '%';
  });
}

function getSummary() {
  return loadResults().summary;
}

function getRecentRuns(count = 10) {
  return loadResults().runs.slice(-count);
}

module.exports = { recordRun, getSummary, getRecentRuns, loadResults };
```
  </action>
  <verify>node -e "const e = require('$HOME/.claude/scripts/hooks/skills/eval/eval-storage'); console.log(e.getSummary())"</verify>
  <done>eval-storage.js tracks runs and calculates pass@k</done>
</task>

<task type="auto">
  <name>Task 2: Create Eval Harness</name>
  <files>~/.claude/scripts/hooks/skills/eval/eval-harness.js</files>
  <action>
Create eval harness runner (~250 LOC):

```javascript
#!/usr/bin/env node
const { execSync } = require('child_process');
const path = require('path');
const { recordRun, getSummary, getRecentRuns } = require('./eval-storage');

class EvalHarness {
  constructor(options = {}) {
    this.suite = options.suite || 'default';
    this.project = options.project || path.basename(process.cwd());
    this.attempt = options.attempt || 1;
    this.command = options.command || this.detectTestCommand();
  }

  detectTestCommand() {
    // Check for package.json test script
    try {
      const pkg = require(path.join(process.cwd(), 'package.json'));
      if (pkg.scripts?.test) return 'npm test';
    } catch {}

    // Check for pytest
    try {
      execSync('which pytest', { stdio: 'pipe' });
      return 'pytest -v';
    } catch {}

    // Check for go test
    try {
      execSync('which go', { stdio: 'pipe' });
      if (require('fs').existsSync('go.mod')) return 'go test ./...';
    } catch {}

    return 'npm test';
  }

  async run() {
    const startTime = Date.now();
    let passed = false;
    let output = '';
    let testCount = { passed: 0, failed: 0, total: 0 };

    try {
      output = execSync(this.command, {
        encoding: 'utf8',
        timeout: 300000, // 5 min max
        stdio: ['pipe', 'pipe', 'pipe'],
        maxBuffer: 10 * 1024 * 1024,
      });
      passed = true;
      testCount = this.parseTestOutput(output);
    } catch (err) {
      output = err.stdout || err.message;
      testCount = this.parseTestOutput(output);
    }

    const duration = Date.now() - startTime;

    const run = recordRun({
      project: this.project,
      suite: this.suite,
      attempt: this.attempt,
      passed,
      total: testCount.total,
      testsPassed: testCount.passed,
      testsFailed: testCount.failed,
      duration,
      command: this.command,
    });

    return {
      run,
      output: this.truncateOutput(output, 50),
      summary: getSummary(),
    };
  }

  parseTestOutput(output) {
    // npm test / jest
    let match = output.match(/Tests:\s*(\d+)\s*passed.*?(\d+)\s*total/);
    if (match) return { passed: parseInt(match[1]), total: parseInt(match[2]), failed: parseInt(match[2]) - parseInt(match[1]) };

    // pytest
    match = output.match(/(\d+)\s*passed.*?(\d+)\s*failed/);
    if (match) return { passed: parseInt(match[1]), failed: parseInt(match[2]), total: parseInt(match[1]) + parseInt(match[2]) };

    // go test
    match = output.match(/ok\s+.*?\s+(\d+)/g);
    if (match) return { passed: match.length, failed: 0, total: match.length };

    return { passed: 0, failed: 0, total: 0 };
  }

  truncateOutput(output, lines) {
    const arr = output.split('\n');
    if (arr.length <= lines) return output;
    return arr.slice(-lines).join('\n');
  }

  formatReport(result) {
    const { run, summary } = result;
    return `
╔═══════════════════════════════════════════╗
║           EVAL HARNESS REPORT             ║
╚═══════════════════════════════════════════╝

Run: ${run.id}
Suite: ${run.suite}
Attempt: ${run.attempt}
Status: ${run.passed ? '✓ PASSED' : '✗ FAILED'}

Tests: ${run.testsPassed}/${run.total} passed
Duration: ${run.duration}ms

━━━ Pass@K Summary (last 100 runs) ━━━
${Object.entries(summary.passAt || {}).map(([k, v]) => `  ${k}: ${v}`).join('\n')}

Total runs: ${summary.totalRuns}
Last updated: ${summary.lastUpdated}
`;
  }
}

// CLI
if (require.main === module) {
  const args = process.argv.slice(2);
  const options = {
    suite: args.find(a => a.startsWith('--suite='))?.split('=')[1],
    attempt: parseInt(args.find(a => a.startsWith('--attempt='))?.split('=')[1]) || 1,
    command: args.find(a => a.startsWith('--command='))?.split('=')[1],
  };

  if (args.includes('--summary')) {
    console.log(JSON.stringify(getSummary(), null, 2));
    process.exit(0);
  }

  if (args.includes('--recent')) {
    console.log(JSON.stringify(getRecentRuns(), null, 2));
    process.exit(0);
  }

  const harness = new EvalHarness(options);
  harness.run().then(result => {
    console.log(harness.formatReport(result));
    process.exit(result.run.passed ? 0 : 1);
  });
}

module.exports = { EvalHarness };
```
  </action>
  <verify>node ~/.claude/scripts/hooks/skills/eval/eval-harness.js --summary</verify>
  <done>eval-harness.js runs tests and tracks pass@k metrics</done>
</task>

<task type="auto">
  <name>Task 3: Create Eval Commands</name>
  <files>~/.claude/commands/eval/run.md, ~/.claude/commands/eval/report.md</files>
  <action>
Create 2 eval commands:

**run.md:**
```markdown
# Run Eval

Execute test suite and track pass@k metrics.

## On Invocation
\`\`\`bash
node ~/.claude/scripts/hooks/skills/eval/eval-harness.js --attempt=$ATTEMPT
\`\`\`

## Options
- `--suite=<name>` - Name this test suite
- `--attempt=<n>` - Which attempt this is (default: 1)
- `--command=<cmd>` - Override test command

## Metrics Tracked
- **pass@1**: First-attempt success rate
- **pass@2**: Success by second attempt
- **pass@k**: Success by attempt k

## Why Track Attempts
When fixing bugs or implementing features, track:
1. First attempt → measures initial code quality
2. Subsequent attempts → measures iteration efficiency

## Example Workflow
\`\`\`bash
# First attempt
/eval:run --attempt=1

# After fix
/eval:run --attempt=2

# Check metrics
/eval:report
\`\`\`
```

**report.md:**
```markdown
# Eval Report

View pass@k metrics summary.

## On Invocation
\`\`\`bash
node ~/.claude/scripts/hooks/skills/eval/eval-harness.js --summary
\`\`\`

## Shows
- pass@1, pass@2, ... pass@k percentages
- Total runs tracked
- Recent run history

## Recent Runs
\`\`\`bash
node ~/.claude/scripts/hooks/skills/eval/eval-harness.js --recent
\`\`\`

## Data Location
- Local: `~/.claude/evals/results.json`
- QuestDB: `claude_eval_runs` table

## Grafana Dashboard
View trends in Grafana at:
- Panel: "Eval Pass@K Trends"
- Query: `SELECT * FROM claude_eval_runs WHERE project = 'your-project'`
```
  </action>
  <verify>ls -la ~/.claude/commands/eval/</verify>
  <done>2 command files created: run.md, report.md</done>
</task>

<task type="auto">
  <name>Task 4: Create Tests</name>
  <files>~/.claude/scripts/hooks/skills/eval/eval.test.js</files>
  <action>
Create test suite (~20 tests):

**Storage tests (8 tests):**
1. recordRun creates run with ID
2. recordRun includes timestamp
3. loadResults returns empty on first run
4. getSummary calculates pass@1
5. getSummary calculates pass@2
6. getRecentRuns limits count
7. Results persist across calls
8. QuestDB export called (mock)

**Harness tests (8 tests):**
1. detectTestCommand finds npm test
2. detectTestCommand finds pytest
3. parseTestOutput handles jest format
4. parseTestOutput handles pytest format
5. run records result
6. run returns output
7. formatReport includes pass@k
8. CLI --summary works

**Integration tests (4 tests):**
1. Full run in test directory
2. Multiple attempts tracked correctly
3. Pass@k calculated correctly over runs
4. Failed run recorded as not passed

Use temp directory and mock test commands.
  </action>
  <verify>node --test ~/.claude/scripts/hooks/skills/eval/eval.test.js</verify>
  <done>20 tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] eval-storage.js tracks runs and calculates pass@k
- [ ] eval-harness.js runs tests and records results
- [ ] /eval:run command works
- [ ] /eval:report shows summary
- [ ] QuestDB export works (or fails gracefully)
- [ ] 20 tests pass
- [ ] Manual test: Run tests twice, check pass@1 and pass@2 calculated
</verification>

<success_criteria>
- Pass@k metrics calculated correctly
- Multiple attempts tracked
- Automatic test command detection
- QuestDB export for Grafana visibility
- Rich terminal report format
- 20+ tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/15-skills-port/15-04-SUMMARY.md`
</output>
